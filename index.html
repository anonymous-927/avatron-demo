<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="3D facial animation, Speech-driven, Emotional, Disentanglement">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EmoTalk</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EmoTalk: Speech-driven emotional disentanglement for 3D face animation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ziqiaopeng.github.io/">Ziqiao Peng</a><sup>1</sup>,</span>
            <span class="author-block">
              <strong>Haoyu Wu</strong><sup>1</sup>,</span>
            <span class="author-block">
              <strong>Zhenbo Song</strong><sup>2</sup>,
            </span>
            <span class="author-block">
              <strong>Hao Xu</strong><sup>3,6</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiangyuzhu-open.github.io/homepage/">Xiangyu Zhu</a><sup>4</sup>
            </span>
            <br>
            <span class="author-block">
              <a href="https://www.sem.tsinghua.edu.cn/info/1189/32080.htm">Hongyan Liu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="http://info.ruc.edu.cn/jsky/szdw/adszycx/bssds/jsjyyjs1/22e455e725db45a3a310bbc0f045c0f1.htm">Jun He</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://fanzhaoxin666.github.io/">Zhaoxin Fan</a><sup>1,6</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Renmin University of China,</span>
            <span class="author-block"><sup>2</sup>Nanjing University of Science and Technology,</span>
            <span class="author-block"><sup>3</sup>The Hong Kong University of Science and Technology,</span>
            <span class="author-block"><sup>4</sup>Chinese Academy of Sciences,</span>
            <span class="author-block"><sup>5</sup>Tsinghua University,</span>
            <span class="author-block"><sup>6</sup>Psyche AI Inc.</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./media/2303.11089.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.11089"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#teaser"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/ZiqiaoPeng/EmoTalk"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset*</span>
                  </a>
            </div>
            <strong>*</strong>
            : will be provided later.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay loop controls playsinline height="100%">
        <source src="./static/videos/emotalk-arxiv.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <strong>EmoTalk</strong> is an end-to-end neural network for generating speech-driven emotion-enhanced 3D facial animation.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven 3D face animation aims to generate realistic facial expressions 
            that match the speech content and emotion. However, existing methods often neglect 
            emotional facial expressions or fail to disentangle them from speech content. 
            <br>
            <br>
            To address this issue, this paper proposes an end-to-end neural network to disentangle 
            different emotions in speech so as to generate rich 3D facial expressions. Specifically, 
            we introduce the emotion disentangling encoder (EDE) to disentangle the emotion and 
            content in the speech by cross-reconstructed speech signals with different emotion labels. 
            Then an emotion-guided feature fusion decoder is employed to generate a 3D talking face 
            with enhanced emotion. The decoder is driven by the disentangled identity, emotional, 
            and content embeddings so as to generate controllable personal and emotional styles. 
            <br>
            <br>
            Finally, considering the scarcity of the 3D emotional talking face data, we resort to 
            the supervision of facial blendshapes, which enables the reconstruction of plausible 
            3D faces from 2D emotional data, and contribute a large-scale 3D emotional talking face 
            dataset (3D-ETF) to train the network. Our experiments and user studies demonstrate 
            that our approach outperforms state-of-the-art methods and exhibits more diverse facial 
            movements. We recommend watching the supplementary video.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    
    <!-- Proposed Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Method</h2>
        <div class="content has-text-justified">
          <img src="./static/videos/emotalk.png" alt="Italian Trulli">
          <p>
            <br>
            Overview of EmoTalk. Given a speech input <img style="transform: translateY(0.1em); background: white;" src="../emotalk/svg/MlAOibqZtk.svg">
            , emotional level <img style="transform: translateY(0.1em); background: white;" src="../emotalk/svg/834tL2PhKo.svg">, and personal style 
            <img style="transform: translateY(0.1em); background: white;" src="../emotalk/svg/d7izxxo4PO.svg"> as inputs,
             our model disentangles the emotion and content in the speech using two latent spaces. The features extracted from these latent spaces are combined
              and fed into the emotion-guided feature fusion decoder, which outputs emotion-enhanced blendshape coefficients. These coefficients can be used to
               animate a FLAME model or rendered as an image sequence.
          </p>
          <!-- <p>
            
          </p>
          <p>
            
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Proposed Method. -->  
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
  @article{peng2023emotalk,
    title={EmoTalk: Speech-driven emotional disentanglement for 3D face animation}, 
    author={Ziqiao Peng and Haoyu Wu and Zhenbo Song and Hao Xu and Xiangyu Zhu and Hongyan Liu and Jun He and Zhaoxin Fan},
    journal={arXiv preprint arXiv:2303.11089},
    year={2023}
  }
</code></pre>
  </div>
</section>
 



<footer class="footer">
  <div class="container">
      <div class="content has-text-centered">
          <a class="icon-link" href="./media/2303.11089.pdf">
              <i class="fas fa-file-pdf"></i>
          </a>
          <a class="icon-link" href="https://github.com/ZiqiaoPeng/EmoTalk" class="external-link" disabled>
              <i class="fab fa-github"></i>
          </a>
      </div>
      <div class="columns is-centered">
          <div class="column is-8">
              <div class="content">
                  <p style="text-align:center">
                    This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  </p>
                  <p style="text-align:center">
                    Website source code based on the <a
                    href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
                  </p>

              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
