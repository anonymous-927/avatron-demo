<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="3D facial animation, Text-driven, Text-to-Speech">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EAVANET</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> <strong>Avatron: 3D Face Avatar and Voice Generation from Text</strong> </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a> </a></span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class ="div_video">
        <iframe width="840" height="470" src="https://www.youtube.com/embed/8-BGlyllSms?si=f1oNwKdxjTYfVWNy" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
      </div>
        <h2 class="subtitle has-text-centered">
        <strong>Avatron</strong> can efficiently generate expressive 3D facial animation from text. 
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Speech-driven 3D facial animation models are pivotal components in a variety of human interaction systems. However, speech-driven method is affected by quality of input speech; low quality speech makes bad quality 3D avatar.
Also, it is impossible to change the content of the input speech of the avatar model unless the text-to-speech (TTS) model which involves an inefficient pipeline: converting text to speech and subsequently generating a 3D avatar from the synthesized speech. It requires huge memory and computational time.
To overcome this constraint, we propose an innovative approach for 3D facial animation that effectively merges these two domains. 
Our model takes advantage of intermediate features derived from the alignment of text and speech, which are then utilized as input for an avatar decoder to transform them into the vertices of a 3D facial animation. We do not need an automatic speech recognition (ASR) model that is usually used in speech-driven approaches to extract context features from speech.
Since our model enables a unified approach that involves a thorough analysis of the interplay between lip movement, text, and speech characteristics, it delivers exceptional performance while substantially reducing both the number of parameters and computational complexity. 
We demonstrate the efficacy of our model by assessing it in terms of the mean square error of vertices and through user study that include ABX test comparisons.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    
    <!-- Proposed Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Method</h2>
        <div class="content has-text-justified">
          <img src="./static/images/avatron_archi.png" alt="Italian Trulli">
          <p>
            <br>
            The emotional speech inputs to the audio encoder to model contextual information, then they are downsampled by convolution layers and fed into the GAU by adding style embedding and speaker embedding. The output of GAU is converted to the vertices of 3D face mesh after passing through BLSTM and projection layers. During training, the style embeddings are estimated from the style vertices of the target emotion but not from the target vertices. In the inference process the center of each emotion cluster is used instead. 

          </p>
          <!-- <p>
            
          </p>
          <p>
            
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Proposed Method. -->  
</section>
 



<footer class="footer">
  <div class="container">
      <div class="content has-text-centered">
          <a class="icon-link" href="./media/2303.11089.pdf">
              <i class="fas fa-file-pdf"></i>
          </a>
          <a class="icon-link" href="https://github.com/ZiqiaoPeng/EmoTalk" class="external-link" disabled>
              <i class="fab fa-github"></i>
          </a>
      </div>
      <div class="columns is-centered">
          <div class="column is-8">
              <div class="content">
                  <p style="text-align:center">
                    This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  </p>
                  <p style="text-align:center">
                    Website source code based on the <a
                    href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
                  </p>

              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
